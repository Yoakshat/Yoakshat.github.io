<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://yoakshat.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://yoakshat.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-11-22T19:30:42+00:00</updated><id>https://yoakshat.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Titanic Rules Frenzy</title><link href="https://yoakshat.github.io/blog/2023/Titanic-Rules-Frenzy/" rel="alternate" type="text/html" title="Titanic Rules Frenzy" /><published>2023-11-22T00:00:00+00:00</published><updated>2023-11-22T00:00:00+00:00</updated><id>https://yoakshat.github.io/blog/2023/Titanic-Rules-Frenzy</id><content type="html" xml:base="https://yoakshat.github.io/blog/2023/Titanic-Rules-Frenzy/"><![CDATA[<h2 id="we-love-rules">We Love Rules</h2>
<p>Predicting who survived on the Titanic. With some plain data science, fun experiments, and RULES, you can be in the top 6% of 15000 teams on Kaggle. I’ll show you how. Click the link below &amp; explore:</p>

<p><a href="https://www.kaggle.com/code/akshatmundra/titanic-with-just-rules?scriptVersionId=151875651">My Kaggle Notebook with a 79.904% accuracy: Can you get it to 80%?</a></p>]]></content><author><name>Akshat Mundra</name></author><category term="math" /><category term="machine-learning" /><summary type="html"><![CDATA[Solving The Classic ML Task With Only Rules]]></summary></entry><entry><title type="html">Learning From A World Health-Educator</title><link href="https://yoakshat.github.io/blog/2023/health-tech-leaders/" rel="alternate" type="text/html" title="Learning From A World Health-Educator" /><published>2023-08-20T00:00:00+00:00</published><updated>2023-08-20T00:00:00+00:00</updated><id>https://yoakshat.github.io/blog/2023/health-tech-leaders</id><content type="html" xml:base="https://yoakshat.github.io/blog/2023/health-tech-leaders/"><![CDATA[<p>Guys, this is a peek into my book and an interview with a leader in health-tech. For anyone who wants to keep up or even learn how to innovate in health-tech, this is for you. A lot of their answers, I’ve paraphrased, but still retain the core message. In my book, I have the TLDR to show the most important insights. Here, 
I’ve also added an extended Q&amp;A. Have fun!</p>

<h2 id="charlotte-cole">Charlotte Cole</h2>
<p>Charlotte Cole is an innovator in education for places with limited resources. She has built education-solutions from Africa to Haiti to Thailand, through the medium of tv-shows to magazines. Specifically in healthcare, she has built characters like Kami and Lola educating young children about HIV to preventing malaria.</p>

<p>She used to be one of the heads of global education at Sesame Workshop in New York City. Now she has co-founded Blue Butterfly 
Collaborative aiming to transform education around the world</p>

<h1 id="tldr">TLDR:</h1>
<ul>
  <li>
    <p>Work with local people. If you try to develop a Whatsapp-education program, in a place where kids rarely have access to a phone it’ll be hard.
Instead a local university nearby might help you deliver your message by airing your program on their computers.</p>
  </li>
  <li>
    <p>Redefine scalability not as the number of people you impact, but how big of an impact you had on a small amount of people. For example, if you can surround these 
people with educational materials from when they’re at a hospital, to when they’re in school, to when they’re taking shelter from an earthquake, you’ll have a big impact.</p>
  </li>
  <li>
    <p>Specialize in one part of the education pipeline, and outsource to organizations good at things you are not. If you’re a content developer, outsource to 
distributors, broadcasters, etc.</p>
  </li>
</ul>

<p><strong>Q: How do you battle health accessibility with health messaging?</strong></p>

<p><strong>A</strong>: In some parts of the world, people don’t have accessibility to basic care, while others don’t have accessibility to sophisticated care. So I think messages do have to be contextually based.</p>

<p>For example, at Sesame we were trying to figure out how to educate kids about nutrition. We thought about showing kids a rainbow of 
different colors, you know, eat your 5 colors. But the problem is 
often kids don’t decide what they eat, they eat what their parents
give them.</p>

<p>This is what we struggle with.</p>

<p><strong>Q: Are there any examples of you have gotten over these barriers?</strong></p>

<p><strong>A</strong>: The best way for the nutrition example above is to 
work with local people. For example, breadfruit is something 
that is readily available. However, many people have stopped 
using it. Encouraging people to go back to this local food, that is more nutritious than a packaged food, is an example.</p>

<p>But to do this, it’s really important to work with local people.</p>

<p><strong>Q: With that, have you faced any challenges with scaling up when a lot of the solutions have to be delivered locally?</strong></p>

<p><strong>A</strong>: Scalability is not necessarily a goal. We do work in media, 
and mass media is really good at reaching a lot of people. But if 
you want to really have an impact, there’s something to be said for
having an impact at a local level. Even if it’s not a lot people, 
you’re impacting them in a big way.</p>

<p>Maybe that’s scalability. It’s all about depth vs breadth.</p>

<p><strong>Q: Do partners and funders believe in the mission of having 
a big impact at the local level?</strong></p>

<p><strong>A</strong>: Funding is always hard, but we’re lucky to have a lot of partners that have a similar mission. For example in Hades, we work as a part of a consortium of nonprofits which are very education-minded.</p>

<p>We’re content developers and that’s our strength. But we have partners that work with schools, are broadcasters, are great distributors, etc. Increasingly, what we’re finding in the difficult places we work, is the more we can specialize on different pieces, the more impact we can collectively have.</p>

<p><strong>Q: 
Do you see one of your goals being to encourage more like minded communities and more grass-root innovators to come into this field OR is it a need for more efficient and better partnerships?</strong></p>

<p><strong>A</strong>: I mean our goal is to build the capacity in local teams to 
develop high-quality educational material for kids. You know, we have some tricks up our sleeve. We can give people content that’s worked in other parts of the world and they can adapt it for their use.</p>

<p>In a lot of school settings, the way kids are taught doesn’t match
what we know about how kids learn. And, and so what we’re trying to do is kind of move from a very didactic way of teaching, to a way that inspires kids to think on their own and think critically, and to collaborate with each other and to think creatively. But in order to do that, you have to change the mindset, and approaches of teachers and even the materials that they’re using.</p>

<p><strong>Q: I’m super interested in the health-education part of your work, but you work in the entirety of education! What’s up with that?</strong></p>

<p><strong>A</strong>: In Haiti, we actually have an animated program of a little
cuckoo. Many of the hospitals in Haiti have it on a loop so while
you’re in the waiting room, you can watch it. And families are stuck in hours in waiting rooms.</p>

<p>And, you know, when they’re showing in a hospital, they’re not necessarily selling health content, they’re just showing our content. We do this kind of thing through food programs too, where when people are distributing food, you can also add activity pages for kids (to learn while eating). Or in emergency situations, where kids are taking shelter for hours, if you have magazines in that shelter you can achieve learning!</p>

<p><strong>Q: How do you evaluate what educational programs work and don’t work?</strong></p>

<p><strong>A</strong>: You have to typically come up with some metric and analyze that. Maybe you expose a group of children to your content versus another type of content, and see what they learned before and after. A lot of the studies are done through ‘randomized control trials.’</p>

<p>And then there’s also the fact that studies might not reflect real life. A Bangladeshi researcher actually observed families as they were engaging with our program, and interviewed them before and after they watched it. You need a combination of real-life observations and experimental data.</p>]]></content><author><name>Akshat Mundra</name></author><category term="healthcare" /><summary type="html"><![CDATA[A conversation with Charlotte Cole]]></summary></entry><entry><title type="html">Health + AI Research</title><link href="https://yoakshat.github.io/blog/2023/health-+-AI-research/" rel="alternate" type="text/html" title="Health + AI Research" /><published>2023-07-29T00:00:00+00:00</published><updated>2023-07-29T00:00:00+00:00</updated><id>https://yoakshat.github.io/blog/2023/health-+-AI-research</id><content type="html" xml:base="https://yoakshat.github.io/blog/2023/health-+-AI-research/"><![CDATA[<p><em>I want to credit my mentor Gabriel Barner Erion for giving me some of these ideas and discussions which led to the spark of my own ideas</em></p>

<h2 id="introduction">Introduction</h2>

<p>Hospitals can benefit from Artificial Intelligence with tasks like predicting if a patient is going to be readmitted or not. For example, if a model outputs yes to be readmitted, hospitals may choose to schedule a primary care visit within a week. This helps both care outcomes and makes sure the hospital is running efficiently. No longer, will that readmitted patient take extra space in the intensive care unit (ICU)!</p>

<p>Using discharge summaries, many machine learning models have been 
trained on this task. Discharge summaries are what a doctor writes just before the patient is discharged, with information 
about the entire patient’s stay. However, often these summaries are templated. Now imagine every time an ice-cream truck comes to the beach, 
the tides grow bigger. It’s a coincidence but if that’s the only thing you saw, the next time you saw an ice-cream truck you’d say the tides are going
to grow bigger. To remove this type of correspondence, this research asks what happens if we remove templates from discharge summaries?</p>

<h2 id="finding-templates">Finding Templates</h2>

<p>Imagine you had a function to determine the similarity between two documents: \(sim(doc1, doc2)\). 
The documents that are the closest together in a representation space, will often be templates.</p>

<h2 id="transforming-text-into-a-representation-space">Transforming Text into a Representation Space</h2>

<p>Machines, computers, and math all understand numbers not words. One way to encode a document is with 
<strong>bag-of-words</strong>, which creates a vector of word frequencies. Imagine you have a dictionary 
like [loves, he, mom].</p>

<p>Your sentence “he loves loves mom” will be transformed into the vector [2, 1, 1]. Let’s show how this can be done in code!</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="n">texts</span><span class="p">){</span>
    <span class="k">return</span> <span class="nc">CountVectorizer</span><span class="p">().</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Then, we might define the similarity function of two vectors as:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="k">def</span> <span class="nf">similarity</span><span class="p">(</span><span class="n">vector1</span><span class="p">,</span> <span class="n">vector2</span><span class="p">){</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">vector2</span> <span class="o">-</span> <span class="n">vector1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">);</span> 
<span class="p">}</span>
</code></pre></div></div>

<p>This is just the euclidean distance between both vectors, but without the square root! Using this code, you can compare one document 
to thousands of other documents. Then, the top ten are likely templates. However, an even better way of finding templates doesn’t require
<strong>bag-of-words</strong>. For finding templates, <strong>one-hot-encoding</strong> actually seems to be better.</p>

<p><strong>One-hot-encoding</strong> means that if the dictionary is [loves, he, mom] and the sentence is “he loves loves mom”, this sentence will be 
transformed into the vector [1, 1, 1]. A word is only counted once. In other words, we only care if a word is present or not present!</p>

<h2 id="algorithm-1">Algorithm 1</h2>

<ol>
  <li>Turn documents into vectors with one-hot-encoding</li>
  <li>Loop through vectors and compute similarities with all others</li>
  <li>If there are over 50 documents that are <strong>highly similar</strong> (greater than some threshold), put these documents into one template</li>
</ol>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">find_templates</span><span class="p">(</span><span class="n">texts</span><span class="p">):</span> 
    <span class="n">templates</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># turns documents into vectors with one-hot-encoding
</span>    <span class="n">vectors</span> <span class="o">=</span> <span class="nf">one_hot_encode</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">vectors</span><span class="p">:</span> 
        <span class="c1"># compute similarity with all vectors
</span>        <span class="n">simScores</span> <span class="o">=</span> <span class="nf">similarityAll</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">vectors</span><span class="p">)</span>
        <span class="c1"># if over 50 documents
</span>        <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">simScores</span> <span class="o">&gt;=</span> <span class="mf">0.9</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">50</span><span class="p">:</span> 
            <span class="c1"># place these documents into one template
</span>            <span class="n">templates</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">texts</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">simScores</span> <span class="o">&gt;=</span> <span class="mf">0.9</span><span class="p">)[</span><span class="mi">0</span><span class="p">]])</span>
</code></pre></div></div>

<p>We actually use a different similarity function than euclidean distance, which proves better at finding templates! Here’s the psuedocode!</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">similarity</span><span class="p">(</span><span class="n">vector1</span><span class="p">,</span> <span class="n">vector2</span><span class="p">):</span> 
    <span class="sh">'''</span><span class="s">
    Take the number of words in document one that document 2 shares and divide it by the unique words of document 1
    Do the vice-versa
    Then average them to get a score
    </span><span class="sh">'''</span>
</code></pre></div></div>

<h2 id="algorithm-2">Algorithm 2</h2>

<ol>
  <li>Turn documents into vectors with one-hot-encoding</li>
  <li>Perform hierarchal clustering onto vectors, which automatically generates templates by merging the most similar vectors at every iteration!</li>
  <li>Create an interactive that lets you visualize templates (I used Dash!) to figure out when you’ve found all the templates</li>
</ol>

<p>There’s a really nice video on hierarchal clustering that’s the best to explain: <a href="https://www.youtube.com/watch?v=7xHsRkOdVwo">StatQuest Video</a></p>

<p>A brief overview of hierarchal clustering is every iteration, it merges the closest groups together! At the end, all the documents will be in one group (it’s like a genetic tree from all the species to Adam and Eve). I make the assumption that because 
<strong>templates will always be the most similar to each other</strong> we can end at the iteration where the next vector or group merged which isn’t a template</p>

<p>Watch the StatQuest video, seriously!</p>

<h2 id="algorithm-3">Algorithm 3</h2>

<p>This algo is very similar to the last one! Hierarchal clustering starts by merging individual vectors and then starts merging groups. Now how do you 
compute the similarity between two groups. You compute the centroid of both groups and then find the similarity between them.</p>

<p>Instead of centroid, in this algorithm we use the mode of the groups. This means if our group was</p>

<p>[1, 1, 0]
[1, 1, 0]
[1, 1, 1]</p>

<p>The mode would be [1, 1, 0]. The reason for using mode is it likely does a better job at recognizing templates. 
Modes are particularly better because [1, 1, 0] represents all the parts that are templated (all the common words). 
Since we are looking for documents that have these common words, we would want to compare it to just these common words!</p>

<p>Also algo 2 used the similarity function of euclidean distance (mainly because it was convenient to plug in scipy’s version of hierarchal clustering 
without having to do much work)! 
Algo 3 uses the same similarity function as algo 1!</p>

<h2 id="results">Results</h2>
<p>A logistic regression model is as good as an XGBoost, or a KNN, to predict readmission. In this case, 
we feed the model <strong>bag-of-words</strong>. How does removing templates via all three algorithms, affect the logistic regression model’s performance?</p>

<p>We actually test two things.</p>
<ol>
  <li>What happens if we remove the templated words inside the document (zero them out in our vector)?</li>
  <li>What happens if we half their word-count (deem them as not as important)?</li>
</ol>

<p><strong>Algorithm 1</strong></p>

<p>V1: removing them completely</p>
<div class="row mt-3">
   <div class="col-sm mt-3 mt-md-0">
       <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/modesfast0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

   </div>
</div>

<p><strong>IMPORTANT</strong></p>

<p>On the x-axis is the ratio of templates:all other documents. The reason for this measurement is most likely, if 
there are a great number of templates relative to other documents, it will hinder the model’s performance. Otherwise, it won’t 
hinder it as much.</p>

<p>On the y-axis is AUROC (Area Under The Receiver Operating Characteristic). The best way to explain is it’s the area under this curve! You can notice why 1 is the best score!</p>
<div class="row mt-3">
   <div class="col-sm mt-3 mt-md-0">
       <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/roc-curve-v2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

   </div>
</div>

<p>The green line represents the versions without templates. The red line represents the versions with templates. 
The dashed lines represent the validation set, the data the model hasn’t seen, while solid lines represent the training set.</p>

<p>What you should be looking for is are the green lines generally at least 5% higher than the red lines?</p>

<p>V2: 50% removal</p>
<div class="row mt-3">
   <div class="col-sm mt-3 mt-md-0">
       <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/modesfast0.5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

   </div>
</div>

<p><strong>Algorithm 2</strong></p>

<p>V1: removing them completely</p>
<div class="row mt-3">
   <div class="col-sm mt-3 mt-md-0">
       <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/dendogram0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

   </div>
</div>

<p>V2: 50% removal</p>
<div class="row mt-3">
   <div class="col-sm mt-3 mt-md-0">
       <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/dendogram0.5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

   </div>
</div>

<p><strong>Algorithm 3</strong></p>

<p>V1: removing them completely</p>
<div class="row mt-3">
   <div class="col-sm mt-3 mt-md-0">
       <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/custom0png.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

   </div>
</div>

<p>V2: 50% removal</p>
<div class="row mt-3">
   <div class="col-sm mt-3 mt-md-0">
       <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/custom0.5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

   </div>
</div>

<h2 id="the-conclusion">The Conclusion</h2>
<p>We seem to have disproved our hypothesis: templates don’t affect machine learning models, even simple like Logistic Regression! 
Well not really, there are still so many new ideas we want to try!</p>

<ol>
  <li>When we actually remove the templated words, we are removing all signs of such word, not just the templated part! Here is an example, 
“I want to bat a tennis ball with a bat” 
“I want to bat a soccer ball with a cat”</li>
</ol>

<p>The template is “I want to bat a _ ball with a _.” We suboptimally, remove all these words. Now when we remove the word, “bat,” in the first sentence shown, 
it also removes the last word of the first sentence (not actually part of the template)!</p>

<p>We still have to figure out how to fix that!</p>

<ol>
  <li>What if we’re not recognizing all the templates? We really want to try <strong>multi-sequence alignment programs</strong>. 
Usually, these programs are used on DNA segments to identify common strands. This is used, for example, to notice 
how similar DNA segments are and build real genetic trees!</li>
</ol>

<p>Now, we want to use this on text! Can it find templates like it finds common strands in DNA?!</p>

<p><strong>We’re not done yet!</strong></p>]]></content><author><name>Akshat Mundra</name></author><category term="math" /><category term="machine-learning" /><summary type="html"><![CDATA[Currently what I am working on...]]></summary></entry><entry><title type="html">A Demo of Word2Vec</title><link href="https://yoakshat.github.io/blog/2023/word2vec-backprop/" rel="alternate" type="text/html" title="A Demo of Word2Vec" /><published>2023-05-09T00:00:00+00:00</published><updated>2023-05-09T00:00:00+00:00</updated><id>https://yoakshat.github.io/blog/2023/word2vec-backprop</id><content type="html" xml:base="https://yoakshat.github.io/blog/2023/word2vec-backprop/"><![CDATA[<p>When you think you know everything about gradient calculus after you learn it in Calc class, you realize
you know nothing. Especially when deriving a derivative for billions of parameters and programming
backpropogation. Libraries like Pytorch and Tensorflow do all the hard backpropogation work for us, 
so why should we care?!</p>

<p>Here is a demo of MathJax showing how to update weights with vectors &amp; gradients.</p>

\[\begin{bmatrix} w_0 &amp; w_1 &amp; w_2 \end{bmatrix} 
    - lr * \begin{bmatrix} 
    \frac{\partial L}{\partial \hat{p_0}}  
    \cr
    \frac{\partial L}{\partial \hat{p_1}}
    \cr
    \frac{\partial L}{\partial \hat{p_2}}
    \end{bmatrix} * 
    \begin{bmatrix}
     \frac{\partial \hat{p_0}}{\partial w_0} &amp; 
     \frac{\partial \hat{p_1}}{\partial w_1} &amp; 
     \frac{\partial \hat{p_2}}{\partial w_2}  
    \end{bmatrix}\]

<p>Now back on topic! In the Natural Language Processing field, years ago, Word2Vec was huge. Now we 
ignore it, having all these fancier-techniques. But TODAY, wanting to go back 
to the basics AND show a behind-the-scenes, here is a demonstration of Word2Vec I created:</p>

<p><a href="https://www.kaggle.com/code/akshatmundra/word2vec-from-almost-scratch">A Kaggle Notebook with a Word2Vec Demo</a></p>]]></content><author><name>Akshat Mundra</name></author><category term="math" /><category term="machine-learning" /><summary type="html"><![CDATA[Going Behind the Scenes]]></summary></entry><entry><title type="html">AI APIs in Healthcare</title><link href="https://yoakshat.github.io/blog/2023/AI-APIs-in-Healthcare/" rel="alternate" type="text/html" title="AI APIs in Healthcare" /><published>2023-04-14T00:00:00+00:00</published><updated>2023-04-14T00:00:00+00:00</updated><id>https://yoakshat.github.io/blog/2023/AI-APIs-in-Healthcare</id><content type="html" xml:base="https://yoakshat.github.io/blog/2023/AI-APIs-in-Healthcare/"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>People are thinking that the new best people in any field will be the people 
who know how to use AI the ‘best way.’ They know how AI can best combine with their creative
mind and efforts. They may be right or absolutely wrong.</p>

<p>AI developers are working to democratize AI, to bring a no-code workspace but still modify the 
AI to more of their liking. To take their expert knowledge and augment the AI with it. Then maybe 
when we get this, physicians and doctors won’t be frustrated that neural networks are ‘black boxes.’ 
Instead they can be like a puppet steering the AI in the right direction and checking if the math 
behind the scenes is leading to the right results.</p>

<p>So as of today, what are the AI APIs in healthcare that do this sort of thing?</p>

<h2 id="technology-apis">Technology APIs</h2>
<p>In the healthcare world, technology API companies are growing and becoming unicorn companies. Optum,
under the same creators of UnitedHealth, is one of the biggest data analytics arms for healthcare 
companies to directly plug in. Healthcare is very complicated, so if a tool doesn’t provide any 
overhead stress but makes life easier, it’s a good pitch.</p>

<p>There are also APIs like Particle Health. As an introduction, healthcare requires great security, 
anonymization, so if we wanted to have an API where hospitals could access patient records just from
their name we wouldn’t be able to. It’d be extremely helpful to hospitals, but also lead to needing
cybersecurity teams constantly battling the hacking going on (while billions of records are at risk). 
What Particle Health does is instead use patient demographics to find a patient and all of their 
records; it’s a great and simple idea!</p>

<p>Brendan Keeler in his new quirky post on April 10 called “Super Integration Fighter III” splits software into the “atomic units,” databases, user interfaces, and system interfaces.</p>

<p>Healthcare tends to outsource a lot of things to APIs (especially now with physician burnout on a high where it’s better for tech to do some of the jobs). Keeler talks about how when we get to the optimal stage, of any hospital or clinic being able to access a patient’s data or use an API that connects all patients, we’ll have to throw away the idea of tech siloes. It’ll be hard but either there’ll need to be a common data format (like FHIR which makes it easy) or even what would be interesting, a way hospitals could easily change the API for their use case with low-code.</p>

<p>I really want to talk about this part, about getting the power of APIs in the hospital’s hands and obviously with AI, because that’s been my craze for a long time and now on everybody else’s trending page.</p>

<h2 id="ai-no-code-software">AI No-Code Software</h2>
<p>After searching around, there is not much research on AI low-code or no-code software IN HEALTHCARE. We’ve heard 
of AI being used for finding similar gene expressions, for making DNA/RNA sequencing cheap, but what about deployment? 
AI will accelerate, but now hasn’t really, but let’s look at how AI low-code software will work practically?</p>

<p>Brainstorming a few applications off my mind</p>
<ul>
  <li>If a doctor doesn’t agree with the AI model say taking an image and predicting cancer, the doctor can type into the AI their
reasons and the AI might augment its prediction (we’re not there yet).</li>
  <li>More API-style, you tell your AI your use case and they look through the EHR to find what information you need (this could be really cool)</li>
  <li>AI APIs don’t just need to be doctor facing but can also be patient-facing helping them distill hard-to-understand medical information</li>
</ul>

<p>With all the processing tasks, my imagination of low-code is there are differences across each clinic? Can you find some way to embed information in the AI that tells them because of this difference, do this task this way. Because there are common use cases, but a billion different data formats, a billion different little things to do. If we get every sort of data to be machine-readable, throw away the idea of a common data format (like FHIR), we’ll let the AI do its work!</p>

<h2 id="conclusion">Conclusion</h2>
<p>I think what this is getting at is what we call multi-task learning in AI. An AI tried out a bunch of different data problems. Now the AI knows I can use how I solved all these other problems, to solve this new problem, even though it looks 
different! And this is how academics learn, how people learn. Researches have tried different neural networks, RNN, ANNs, Transformers, hundreds of times, and they have some intuition on what model they should use for a new problem.</p>

<p>Instead of the billions of different AI applications we’re seeing (like AI art-generation, AI text-generation, AI Q&amp;A) I think we achieve real human-computer interaction when you can talk to it in some sort of semi machine-human language (not code) and it can do whatever you want.</p>

<p>Then again, I wouldn’t think it would be frustrating if it was like a vending machine or a drink machine, where you click what application you want (text generation, art generation, Q&amp;A). That’ll only be $1.50 (vending machine joke). Then the machine figures how to do the task for your set of data. It’s all interesting, futuristic, and awesome!</p>

<p><i>Goodnight even though it’s not night,
yall!</i></p>]]></content><author><name>Akshat Mundra</name></author><category term="healthcare" /><category term="machine-learning" /><summary type="html"><![CDATA[how AI is being brought to medical experts with no AI expertise]]></summary></entry><entry><title type="html">Unsupervised learning is weird!</title><link href="https://yoakshat.github.io/blog/2023/unsupervised-learning-is-weird/" rel="alternate" type="text/html" title="Unsupervised learning is weird!" /><published>2023-04-12T00:00:00+00:00</published><updated>2023-04-12T00:00:00+00:00</updated><id>https://yoakshat.github.io/blog/2023/unsupervised-learning-is-weird</id><content type="html" xml:base="https://yoakshat.github.io/blog/2023/unsupervised-learning-is-weird/"><![CDATA[<p>Unsupervised learning is a very weird world. Imagine you are walking
down a park, sit on a bench, and then get some wise words from an older person.
The words don’t make any sense, but somehow they give structure to your life. I am
not sure many of you have had that experience, but unsupervised learning uses nothing
(except some scientist’s statistical techniques) to find structure in the data.</p>

<p>Monty Python (oops I mean Monty Hall) is also a weird problem but statistics tell us the answer.
In this post, I’m going to pull infamous examples of unsupervised learning that have blown scientists
away!</p>

<div class="row mt-3">
   <div class="col-sm mt-3 mt-md-0">
       <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/montyhall.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

   </div>
</div>

<h1>tSNE embeddings with Brawl Stars</h1>

<p>One interesting unsupervised technique is <a href="https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf">tSNE embeddings</a>. The S stands for stochastic, which means an ‘estimation.’ The classic example is a ball rolling down a hill, but you have no idea where the ball is going to end up. Now comes an example related to what we love, playing Brawl Stars! As a brawler, you’re noticing hmmm let’s draw a boundary here. After all, we can’t have Crow
and Leon attacking each other right in the start! Now let’s continue drawing boundaries, until we have a pretty fair map!</p>

<p>For the non Brawl-Star readers, first of all, I’m sorry. Hopefully you’ve heard of the game Catan. Everyone knows
the process of changing the hexagons so no stones are nearby, no 3 adjacent hexagons are too powerful. That is a
‘stochastic’ description of how tSNE works, but instead of setting the Catan board up or creating a Brawl Stars map,
it’s taking high-dimensional data and transforming it into low-dimensional data. The idea is that data points nearby
in high dimensions should also be nearby in the transformed dimensions (e.g. Leon and Leon should be close together,
but not Leon and Crow).</p>

<p>A really nice explanation with visualizations is <a href="https://www.enjoyalgorithms.com/blog/tsne-algorithm-in-ml">here</a>.</p>

<div class="row mt-3">
   <div class="col-sm mt-3 mt-md-0">
       <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/brawl.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

   </div>
</div>

<h5>What has tSNE been used for?</h5>
<p>Imagine <a href="https://www.nature.com/articles/s41467-019-13056-x">RNA expression levels for thousands of genes from up to millions of cells”</a>. How do you compare single cells with millions of information? Think about why tSNE is there, because ‘life’ is very complicated (get it haha). And maybe if you find a single cell that on a 2D graph
is near another one that scientists know, you can tell that cell’s purpose. Maybe inside the cell, a gene is being
regulated that is causing a disease.</p>

<p>Or maybe <a href="https://www.kaggle.com/code/vatsalmavani/music-recommendation-system-using-spotify-dataset">Spotify</a> wants to take the audio features of the song, and based on the songs you like, tell you what other songs you’ll like (e.g. sound similar). Or even maybe Spotify wants to give you vastly different song recs so you can explore your music
taste!</p>

<h2>Zero shot learning</h2>
<p>I don’t know if zero shot learning is an example of unsupervised learning, but the idea is that it
can learn new concepts without have any data. So for example when you were a baby, you saw penguins, koalas,
snakes, pandas, but you never saw a Giraffe. You didn’t even know it was called a Giraffe! So you decided to
call it tall white animal with golden spots.</p>

<p>You don’t need any labels (kind of the idea of unsupervised learning). This paper in 2015 that I have absolutely
not read, but <a href="https://proceedings.mlr.press/v37/romera-paredes15.html">caught my attention</a> because it
discusses how a one-line solution beat SOTA in zero-shot learning. It’s definitely interesting, but that’s all I want
to talk about it for now!</p>

<div class="row mt-3">
   <div class="col-sm mt-3 mt-md-0">
       <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/v7labs.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

   </div>
</div>

<h2>Let's talk about Autoencoders!</h2>
<p>Have I ever heard anyone that excited about autoencoders?! Actually, yes. <a href="http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/">A nice read on autoencoders</a>.</p>

<p>Even autoencoders would probably be excited about themselves because they love to learn! Let me explain. The sort of
motivation goes like we love discovering things ourselves without anyone’s help. Let’s give you a neural network and
all you have to do is take your image and output that same image.</p>

<p>But we’re going make life very hard for you! We’re going to add an activation (so you can’t just have weights with 1,
1, 1, 1), we’re going to turn off some of your neurons. It’s hell, but by trying to get out of that hell, the autoencoder learns hey, maybe, if I figure this is grass most of the pixels around are grass. And so on.</p>

<div class="row mt-3">
   <div class="col-sm mt-3 mt-md-0">
       <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/hell.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

   </div>
</div>

<p>Unsupervised learning is a very weird statistical world! Put in an input of the Multiverse, you might get an output
of the Rick and Morty-verse. It’s a magical thing!</p>

<p><i>Goodnight even though it’s not night,
yall!</i></p>]]></content><author><name>Akshat Mundra</name></author><category term="math" /><category term="machine-learning" /><summary type="html"><![CDATA[just talking about machine learning]]></summary></entry><entry><title type="html">Substack and healthcare</title><link href="https://yoakshat.github.io/blog/2023/substack-and-healthcare/" rel="alternate" type="text/html" title="Substack and healthcare" /><published>2023-02-02T01:51:41+00:00</published><updated>2023-02-02T01:51:41+00:00</updated><id>https://yoakshat.github.io/blog/2023/substack-and-healthcare</id><content type="html" xml:base="https://yoakshat.github.io/blog/2023/substack-and-healthcare/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[WE NEED LABOR UNIONS ON SUBSTACK... PLS DON'T BAN THIS POST]]></summary></entry><entry><title type="html">Competing incentives in healthcare</title><link href="https://yoakshat.github.io/blog/2023/competing-incentives-in-healthcare/" rel="alternate" type="text/html" title="Competing incentives in healthcare" /><published>2023-01-26T14:59:37+00:00</published><updated>2023-01-26T14:59:37+00:00</updated><id>https://yoakshat.github.io/blog/2023/competing-incentives-in-healthcare</id><content type="html" xml:base="https://yoakshat.github.io/blog/2023/competing-incentives-in-healthcare/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[...doctors are not just evil aliens who want run every possible test to get the most money (they are just optimized to do so)]]></summary></entry><entry><title type="html">Update on being gone….</title><link href="https://yoakshat.github.io/blog/2023/update-on-being-gone/" rel="alternate" type="text/html" title="Update on being gone…." /><published>2023-01-24T06:12:31+00:00</published><updated>2023-01-24T06:12:31+00:00</updated><id>https://yoakshat.github.io/blog/2023/update-on-being-gone</id><content type="html" xml:base="https://yoakshat.github.io/blog/2023/update-on-being-gone/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Could America’s Healthcare System Become the Netherlands</title><link href="https://yoakshat.github.io/blog/2023/could-americas-healthcare-system-become-the-netherlands/" rel="alternate" type="text/html" title="Could America’s Healthcare System Become the Netherlands" /><published>2023-01-22T19:34:45+00:00</published><updated>2023-01-22T19:34:45+00:00</updated><id>https://yoakshat.github.io/blog/2023/could-americas-healthcare-system-become-the-netherlands</id><content type="html" xml:base="https://yoakshat.github.io/blog/2023/could-americas-healthcare-system-become-the-netherlands/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Everyone on Reddit says it's impossible, because of America's open market policies. However, Netherlands has healthcare + majorly private markets following similar capitalistic policies]]></summary></entry></feed>